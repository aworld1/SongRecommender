{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlf7-Ia7DFYV"
      },
      "source": [
        "#Modeling\n",
        "\n",
        "In this section, we propose several models to predict whether or not a song has been listened to. We begin with the implementation of several baselines, and proceed with a more advanced model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDhbyDqB-hh",
        "outputId": "c55f730e-c28f-4acc-bafc-2b8a62917ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.7/dist-packages (2.21.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (2.28.1)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (1.26.13)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from spotipy) (4.3.5)\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.1.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.3->spotipy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.3->spotipy) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nimfa in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textdistance\n",
            "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spotipy\n",
        "!pip install nimfa\n",
        "!pip install textdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "id": "3IK293-hyAgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import spotipy\n",
        "import nimfa\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from textdistance import jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "VFEBc8coe7fp"
      },
      "outputs": [],
      "source": [
        "def build_relevant_ds(songs: list):\n",
        "    \"\"\"\n",
        "    Preprocesses data, simultaneously building relevant data structures\n",
        "    \n",
        "    @param data - a data list of playlist dictionaries to preprocess\n",
        "    @returns a list of tracks per user, users per track, watered down data list\n",
        "    \"\"\"\n",
        "    \n",
        "    def process_uri(uri:str):\n",
        "        \"\"\"URI Processing method\"\"\"\n",
        "        return uri.split(\":\")[2]\n",
        "        \n",
        "    print(\"Preprocessing started...\")\n",
        "    tracks_per_user, users_per_track, users_per_artist, artists_per_user = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "    \n",
        "    # Traversing through data and preprocessing\n",
        "    for song in songs:\n",
        "\n",
        "      # Obtaining user\n",
        "      user = song['user']\n",
        "\n",
        "      # obtaining necessary data\n",
        "      track, artist, album = song['track_name'], song['artist_name'], song['album_name']\n",
        "      \n",
        "      # Appending data to data structures\n",
        "      tracks_per_user[user].append(track)\n",
        "      users_per_track[track].append(user)\n",
        "      users_per_artist[artist].append(user)\n",
        "      artists_per_user[user].append(artist)\n",
        "            \n",
        "    return tracks_per_user, users_per_track, users_per_artist, artists_per_user\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "yl6pmXmgDpWG"
      },
      "outputs": [],
      "source": [
        "# Reading in training and testing data\n",
        "with open('data_train.json', 'r') as train_reader:\n",
        "  data_train_val = json.load(train_reader)\n",
        "\n",
        "with open('data_test.json', 'r') as test_reader:\n",
        "  data_test= json.load(test_reader) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "kquJLueVIoiH"
      },
      "outputs": [],
      "source": [
        "data_train, data_val = data_train_val[:600_000], data_train_val[600_000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tm-cp11h6by",
        "outputId": "28cda097-db37-40c9-98dc-3ba04ccb4cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ],
      "source": [
        "val_tracks_per_user, val_users_per_track, val_users_per_artist, val_artists_per_user = build_relevant_ds(data_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMZV5f21b4CH"
      },
      "source": [
        "#### Validation, Test Set Construction\n",
        "\n",
        "The Validation and Test set are balanced sets of positive and negative instances, maintaining 1/2 positive user-item pairs (the user listened to the song) and 1/2 negative user-item pairs (the user did not listen to the song)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "BNCSPXAHPCUS"
      },
      "outputs": [],
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "val_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_val:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['track_name'] in val_tracks_per_user[user]:\n",
        "        random_song = data_val[np.random.randint(0, len(data_val))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "\n",
        "    # Group user with negative example\n",
        "    neg_ex['user'] = user\n",
        "    neg_ex['playlist_name'] = ex['playlist_name']\n",
        "    \n",
        "    # Appending\n",
        "    val_neg_ex.append(neg_ex)\n",
        "\n",
        "data_val += val_neg_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4gy-TSglKEe",
        "outputId": "0a8a143c-b245-4caa-8632-a84d676b173d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ],
      "source": [
        "test_tracks_per_user, test_users_per_track, test_users_per_artist, test_artists_per_user = build_relevant_ds(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "HmGyo6m_jrrw"
      },
      "outputs": [],
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "test_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_test:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['track_name'] in test_tracks_per_user[user]:\n",
        "        random_song = data_test[np.random.randint(0, len(data_test))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "\n",
        "    # Group user with negative example\n",
        "    neg_ex['user'] = user\n",
        "    neg_ex['playlist_name'] = ex['playlist_name']\n",
        "    \n",
        "    # Appending\n",
        "    test_neg_ex.append(neg_ex)\n",
        "\n",
        "data_test += test_neg_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "bs-xyAGYN7hr"
      },
      "outputs": [],
      "source": [
        "# Storing data as dataframe for easier use\n",
        "train_df = pd.DataFrame.from_records(data_train)\n",
        "val_df = pd.DataFrame.from_records(data_val)\n",
        "test_df = pd.DataFrame.from_records(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcT7ukUJJuHi",
        "outputId": "ada27b9d-a13b-4a56-b05e-82f0d5f3bde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Length: 400000, Test Length: 400000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Validation Length: {len(data_val)}, Test Length: {len(data_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mEsLB_dC7gS"
      },
      "source": [
        "##Baseline Models\n",
        "\n",
        "We implement the following baseline models, and attempt to exceed the performance of both:\n",
        "\n",
        "1. Totally Naive Baseline: Prediction by Popularity\n",
        "2. Standard Tier Baseline: Collaborative Filtering\n",
        "3. Supposedly good baseline: Personalized Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6chZ25OgCqoU",
        "outputId": "39fc2d8e-5eba-4bab-c1e4-c695b93905c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ],
      "source": [
        "# Necessary data structures\n",
        "tracks_per_user, users_per_track, users_per_artist, artists_per_user = build_relevant_ds(data_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5mQbLFOYPRa"
      },
      "source": [
        "### Motivating Naive Basline: Prediction by Popularity (Unpersonalized Recommendation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "F_PKbJKGEzbC"
      },
      "outputs": [],
      "source": [
        "def predict_by_pop(tracks: np.array, most_popular: set):\n",
        "  \"\"\"Predicts that a song was listened to if it was among the most popular songs\"\"\"\n",
        "  return [True if track in most_popular else False for track in tracks]\n",
        "\n",
        "def construct_most_popular(users_per_track: list) -> set:\n",
        "    \"\"\"Naive Baseline: Predicts track has been listened to by user if it's in the tracks that account for top 1/2 of listens\"\"\"\n",
        "    # Most popular list init\n",
        "    most_popular = []\n",
        "\n",
        "    # Sorting tracks by popularity\n",
        "    track_popularities = [(len(users_per_track[track]), track) for track in users_per_track]\n",
        "    track_popularities.sort(reverse = True)\n",
        "\n",
        "    # Computing half of total listens\n",
        "    half_tot_popularity = sum([len(users_per_track[track]) for track in users_per_track]) // 2\n",
        "    \n",
        "    # init cumulative popularity\n",
        "    cum_pop, counter = 0,0\n",
        "\n",
        "    # While haven't accounted for half of total listens\n",
        "    while(cum_pop < half_tot_popularity):\n",
        "      # Appending song and adjusting iterators\n",
        "      most_popular.append(track_popularities[counter][1])\n",
        "      cum_pop += track_popularities[counter][0]\n",
        "      counter += 1\n",
        "\n",
        "    return set(most_popular)\n",
        "\n",
        "\n",
        "def acc(labels: np.array, predictions: np.array):\n",
        "  \"\"\"Accuracy computation\"\"\"\n",
        "  return sum(predictions == labels) / len(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUaKjuifE2dh"
      },
      "outputs": [],
      "source": [
        "most_popular = construct_most_popular(users_per_track)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5WToG0HKak",
        "outputId": "4d6e9566-aade-4b4e-cf24-a1fb8ebac086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Accuracy: 0.499755\n"
          ]
        }
      ],
      "source": [
        "val_predictions = predict_by_pop(val_df['track_name'], most_popular)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwHI4SILc0K",
        "outputId": "7b176d2a-75e8-4fe2-8649-cc2c26c89257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.50026\n"
          ]
        }
      ],
      "source": [
        "test_predictions = predict_by_pop(test_df['track_name'], most_popular)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWdXWeibQbGh"
      },
      "source": [
        "### Conclusions from extremely naive Baseline\n",
        "\n",
        "Due to the construction of our validation and test set, prediction by popularity is an entirely ineffective way to build a decision boundary. \n",
        "\n",
        "Although the top 55% (55% was the optimal validation parameter) of Popular songs comprise more of the positive instances, they also comprise more of the equivalent number of randomly sampled negative instances. \n",
        "\n",
        "Since we were just as likely to sample a popular song as a positive instance as we were to sample a popular song as a negative instance, prediction by popularity provides no new information. This motivates that all improvement over random chance can **only** be accounted for by personalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzxD3Gl5dXeg"
      },
      "source": [
        "### Basic Personalization Basline: Prediction by Collaborative Filtering (Personalized Recommendation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "id": "JaSwpZ0BQoo8"
      },
      "outputs": [],
      "source": [
        "def jaccard(set1: set, set2: set):\n",
        "  \"\"\"Jaccard similarity metric\"\"\"\n",
        "  inter = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "  return inter / union if union !=0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8ij6vyUa7iN"
      },
      "outputs": [],
      "source": [
        "def collab_filter_predict(users: list, tracks: list, tracks_per_user: dict, users_per_track:dict):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar users\"\"\"\n",
        "  predictions = []\n",
        "  iter = 0\n",
        "  for track_user, track in zip(users, tracks):\n",
        "\n",
        "    # Similarity vector\n",
        "    similarities = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      similarities.append(jaccard(other_track_users, track_users))\n",
        "\n",
        "    predictions.append(True if (len(similarities) != 0 and max(similarities) > 0.005) else False)\n",
        "  \n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQS4eekHa8FT",
        "outputId": "264fc7d1-4122-472a-93f3-63e854fee5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Accuracy: 0.663725\n"
          ]
        }
      ],
      "source": [
        "val_predictions = collab_filter_predict(val_df['user'], val_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxACLAOPa8Z2",
        "outputId": "52e3a480-c0f6-4295-b485-e50dd6fac9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6981475\n"
          ]
        }
      ],
      "source": [
        "test_predictions = collab_filter_predict(test_df['user'], test_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4Ha81P0-BD"
      },
      "source": [
        "This thresholded collaborative filtering already does pretty well! By leveraging the properties of the validation and test sets (half of the examples are positive, so we can utilize only our 200_000 most positive predictions) let's check if we can do better by utilizing this ranking approach!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fogXbaW7a8kg"
      },
      "outputs": [],
      "source": [
        "def collab_filter_predict_structurally(users: list, tracks: list, tracks_per_user: dict, users_per_track:dict):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar users\"\"\"\n",
        "  predictions = []\n",
        "  max_similarities = []\n",
        "  for track_user, track in zip(users, tracks):\n",
        "\n",
        "    # Similarity vector\n",
        "    sims = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      sims.append(jaccard(other_track_users, track_users))\n",
        "\n",
        "    # If other tracks exist for user, append compute\n",
        "    max_similarities.append(max(sims) if len(sims) > 0 else 0)\n",
        "  \n",
        "  sorted_max_sims = sorted(max_similarities, reverse=True)\n",
        "  confidence_thresh = sorted_max_sims[len(max_similarities)//2]\n",
        "\n",
        "  predictions = [True if sim > confidence_thresh else False for sim in max_similarities]\n",
        "  \n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uaBlK9sa85h",
        "outputId": "d0a2794d-db58-4540-e508-e5ca862e972a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Accuracy: 0.6425425\n"
          ]
        }
      ],
      "source": [
        "val_predictions = collab_filter_predict_structurally(val_df['user'], val_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE2zMnX5223V",
        "outputId": "03f614a3-5861-4a97-cd59-02b442486042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.690045\n"
          ]
        }
      ],
      "source": [
        "test_predictions = collab_filter_predict_structurally(test_df['user'], test_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnyGVZI19rc6"
      },
      "source": [
        "Somehow, adding structure did not make our model better. This could be due to several factors. For one, we suffer from a massive sparsity problem: many of our tracks have very few and hence only 0 overlap. This is part of the cold start problem: it is very unlikely that tracks with very few listeners will overlap with other tracks. We must figure out a better way to handle such tracks, as they are an integral part of our recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjv3TWTI6Bc3"
      },
      "source": [
        "### Model Basline: Logistic Regression\n",
        "We hope to also beat primitive regression modeling, as even with similarity metrics it is not meant to account particularly for interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufEO9yH0fYBD",
        "outputId": "19fc7a62-8a14-45be-8141-ed2aec0c35b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ],
      "source": [
        "tracks_per_user, users_per_track, users_per_artist, artists_per_user, durations_per_user = build_relevant_ds_regression(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvrDc0SlPmGO"
      },
      "outputs": [],
      "source": [
        "def build_relevant_ds_regression(songs: list):\n",
        "    \"\"\"\n",
        "    Preprocesses data, simultaneously building relevant data structures\n",
        "    \n",
        "    @param data - a data list of playlist dictionaries to preprocess\n",
        "    @returns a list of tracks per user, users per track, watered down data list\n",
        "    \"\"\"\n",
        "    \n",
        "    def process_uri(uri:str):\n",
        "        \"\"\"URI Processing method\"\"\"\n",
        "        return uri.split(\":\")[2]\n",
        "        \n",
        "    print(\"Preprocessing started...\")\n",
        "    tracks_per_user, users_per_track, users_per_artist, artists_per_user, durations_per_user = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "    \n",
        "    # Traversing through data and preprocessing\n",
        "    for song in songs:\n",
        "\n",
        "      # Obtaining user\n",
        "      user = song['user']\n",
        "\n",
        "      # obtaining necessary data\n",
        "      track, artist, album, duration = song['track_name'], song['artist_name'], song['album_name'], song['duration_ms']\n",
        "      \n",
        "      # Appending data to data structures\n",
        "      tracks_per_user[user].append(track)\n",
        "      users_per_track[track].append(user)\n",
        "      users_per_artist[artist].append(user)\n",
        "      artists_per_user[user].append(artist)\n",
        "      durations_per_user[user].append(duration)\n",
        "\n",
        "            \n",
        "    return tracks_per_user, users_per_track, users_per_artist, artists_per_user, durations_per_user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE2oUNC-6g9j"
      },
      "outputs": [],
      "source": [
        "def advanced_linear_model(df, tracks_per_user, users_per_track, durations_per_user):\n",
        "  \"\"\"Linear Accounts for Jaccard similarity, but also other factors\"\"\"\n",
        "  # Accounts for Jaccard similarity, but also other factors\n",
        "  feature_vector, labels = linear_feature(df, tracks_per_user, users_per_track, durations_per_user)\n",
        "  model = LogisticRegression()\n",
        "  model.fit(feature_vector,labels)\n",
        "  return model, feature_vector\n",
        "\n",
        "def linear_feature(df, tracks_per_user, users_per_track, durations_per_user):\n",
        "  \"\"\"Creates linear model features\"\"\"\n",
        "  feature_vector = []\n",
        "\n",
        "  for index, record in df.iterrows():\n",
        "    track_user = record['user']\n",
        "    track = record['track_name']\n",
        "\n",
        "    # Similarity vector\n",
        "    similarities = []\n",
        "    ex_features = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      similarities.append(jaccard(other_track_users, track_users))\n",
        "    \n",
        "    # append similarity\n",
        "    ex_features.append(max(similarities) if len(similarities) != 0 else 0)\n",
        "    ex_features.append(int(len(similarities)!=0))\n",
        "\n",
        "    # Appending example features\n",
        "    feature_vector.append(ex_features)\n",
        "\n",
        "  return feature_vector, df['listened']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTCS3ts1dBHx"
      },
      "outputs": [],
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "train_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_train:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['track_name'] in val_tracks_per_user[user]:\n",
        "        random_song = data_train[np.random.randint(0, len(data_val))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "\n",
        "    # Group user with negative example\n",
        "    neg_ex['user'] = user\n",
        "    \n",
        "    # Appending\n",
        "    train_neg_ex.append(neg_ex)\n",
        "\n",
        "data_train += train_neg_ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3RuIUFBdlQR"
      },
      "outputs": [],
      "source": [
        "reg_train_df = pd.DataFrame.from_records(data_train).iloc[np.random.randint(0, len(data_train), 100_000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT779XZqdqEc"
      },
      "outputs": [],
      "source": [
        "model, feature_vector = advanced_linear_model(reg_train_df, tracks_per_user, users_per_track, durations_per_user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLnZ2hxohOWS",
        "outputId": "514d6d0b-1443-4737-e4c9-23188ed43213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.50202\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train Accuracy: {acc(model.predict(feature_vector), reg_train_df['listened'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCxuiMb7kwCU"
      },
      "outputs": [],
      "source": [
        "test_features, test_labels = linear_feature(test_df, tracks_per_user, users_per_track, durations_per_user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Zi51a2kw4W",
        "outputId": "8a5b4aae-a59c-40cf-f999-65e8ce6e3910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.4986575\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test Accuracy: {acc(model.predict(test_features), test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrv4gq_8qGM3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkqmhkEUtFHd"
      },
      "source": [
        "## The Actual Model: Collaborative Filtering++\n",
        "\n",
        "Our actual model upgrades the Personalization effect. We opt to utilize 2 collaborative filtering models, which we will weight by their validation accuracies, to compute a new set of predictions for items for which collaborative data exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "id": "OB-bitd6rHbA"
      },
      "outputs": [],
      "source": [
        "def collab_filter_predict(users: list, tracks: list, tracks_per_user: dict, users_per_track:dict, title_content_preds: list):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar users\"\"\"\n",
        "  predictions = []\n",
        "  iter = 0\n",
        "  for track_user, track, pred in zip(users, tracks, title_content_preds):\n",
        "    iter += 1\n",
        "    if (iter % 30_000 == 0):\n",
        "      print(\"iter: \" + str(iter))\n",
        "\n",
        "    # Similarity vector\n",
        "    similarities = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks[:20]:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      similarities.append(jaccard(other_track_users, track_users))\n",
        "\n",
        "    similarity_max = max(similarities) if len(similarities) != 0 else pred\n",
        "    predictions.append(similarity_max)\n",
        "  \n",
        "  return predictions\n",
        "\n",
        "\n",
        "def artist_collab_filter_predict(users: list, artists: pd.Series, artists_per_user: dict, users_per_artist:dict, title_content_preds: list):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar artist\"\"\"\n",
        "  predictions = []\n",
        "  iter = 0\n",
        "  for track_user, track_artist, content_pred in zip(users, artists, title_content_preds):\n",
        "    # Similarity vector\n",
        "    similarities = []\n",
        "\n",
        "    iter += 1\n",
        "    if (iter % 20_000 == 0):\n",
        "      print(\"iter: \" + str(iter))\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_artists = artists_per_user[track_user]\n",
        "    artists_users = set(users_per_artist[track_artist])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_artist in user_artists[:5]:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_artist == track_artist:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_artist_users= set(users_per_artist[external_artist])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      similarities.append(jaccard(other_artist_users, artists_users))\n",
        "\n",
        "    # Personalized recommendation when data, \n",
        "    similarity_max = max(similarities) if len(similarities) != 0 else content_pred\n",
        "    predictions.append(similarity_max)\n",
        "  \n",
        "  return predictions\n",
        "\n",
        "\n",
        "def title_content_model(playlist_names: str, track_names: str, album_names: str):\n",
        "  \"\"\"Attempting modeling utilizing sentiment analysis\"\"\"\n",
        "  predictions = []\n",
        "  for playlist_name, track_name, album_name in zip(playlist_names, track_names, album_names):\n",
        "    #  Triviality\n",
        "    if album_name == playlist_name or track_name == playlist_name:\n",
        "      predictions.append(1)\n",
        "    else:\n",
        "      # Prediction by jaccard distance based on playlist title\n",
        "      min_text_distance = min(jaccard.distance(playlist_name, track_name), jaccard.distance(playlist_name, album_name))\n",
        "      predictions.append((1-min_text_distance) / 20)\n",
        "\n",
        "  return predictions\n",
        "        \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "id": "6TeSrzkvJe-8"
      },
      "outputs": [],
      "source": [
        "def ensemble_predictions_tune(predictions1: np.array, a1: float, predictions2: np.array, a2: float, val_labels: pd.Series):\n",
        "  \"\"\"\"Ensembles predictions from 3 different models, leverages dataset information to predict 1/2 positives\"\"\"\n",
        "  new_pred = predictions1*a1 + predictions2 * a2\n",
        "  max_acc, max_thresh = 0,0\n",
        "\n",
        "  for threshold in np.arange(0, 0.2, 0.0025):\n",
        "    # Computing accuracy\n",
        "    accuracy = acc(new_pred > threshold, val_labels)\n",
        "    \n",
        "    # Updating threshold if need be\n",
        "    if accuracy > max_acc:\n",
        "      max_acc = accuracy\n",
        "      max_thresh = threshold\n",
        "\n",
        "  # Returning threshold\n",
        "  return max_acc, max_thresh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_content_preds = title_content_model(val_df['playlist_name'], val_df['track_name'], val_df['album_name'])"
      ],
      "metadata": {
        "id": "P6o7fHZzP9-d"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_max_sims_artist = artist_collab_filter_predict(val_df['user'], val_df['artist_name'], artists_per_user, users_per_artist, title_content_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-6PfZHITUll",
        "outputId": "1b19d12a-1f37-42d8-9cef-9c3e42e5e4dc"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n",
            "40000\n",
            "60000\n",
            "80000\n",
            "100000\n",
            "120000\n",
            "140000\n",
            "160000\n",
            "180000\n",
            "200000\n",
            "220000\n",
            "240000\n",
            "260000\n",
            "280000\n",
            "300000\n",
            "320000\n",
            "340000\n",
            "360000\n",
            "380000\n",
            "400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_max_sims_track = collab_filter_predict(val_df['user'], val_df['track_name'], tracks_per_user, users_per_track, title_content_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMwWbA8KgtUV",
        "outputId": "b3983a6b-ffa6-4012-fc8d-01060b1de274"
      },
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 30000\n",
            "iter: 60000\n",
            "iter: 90000\n",
            "iter: 120000\n",
            "iter: 150000\n",
            "iter: 180000\n",
            "iter: 210000\n",
            "iter: 240000\n",
            "iter: 270000\n",
            "iter: 300000\n",
            "iter: 330000\n",
            "iter: 360000\n",
            "iter: 390000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_artist = acc(np.array(val_max_sims_artist) > 0.01, val_df['listened'])"
      ],
      "metadata": {
        "id": "sQiXXbN6hupZ"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_track = acc(np.array(val_max_sims_track) > 0.01, val_df['listened'])"
      ],
      "metadata": {
        "id": "-ZN4Wlu5mUG0"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning predictions\n",
        "new_val_acc, new_val_threshold = ensemble_predictions_tune(np.array(val_max_sims_artist), acc_artist, np.array(val_max_sims_track), acc_track, val_df['listened'])"
      ],
      "metadata": {
        "id": "bYb7qol9rl1d"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_val_threshold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T54Cr3aGxVyV",
        "outputId": "3b648ce3-d8ca-49c4-8ffd-56936416049e"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.015"
            ]
          },
          "metadata": {},
          "execution_count": 436
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jso-8Vj_r98L",
        "outputId": "060e9754-9e4a-46e2-d0a6-5ef7554054fc"
      },
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7629075"
            ]
          },
          "metadata": {},
          "execution_count": 408
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_content_preds_test = title_content_model(test_df['playlist_name'], test_df['track_name'], test_df['album_name'])"
      ],
      "metadata": {
        "id": "MPz8ldTisP5h"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_max_sims_artist = artist_collab_filter_predict(test_df['user'], test_df['artist_name'], artists_per_user, users_per_artist, title_content_preds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D1auEJPtjMH",
        "outputId": "f2f89b4b-1712-4eeb-af85-4eba6fafa262"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter20000\n",
            "iter40000\n",
            "iter60000\n",
            "iter80000\n",
            "iter100000\n",
            "iter120000\n",
            "iter140000\n",
            "iter160000\n",
            "iter180000\n",
            "iter200000\n",
            "iter220000\n",
            "iter240000\n",
            "iter260000\n",
            "iter280000\n",
            "iter300000\n",
            "iter320000\n",
            "iter340000\n",
            "iter360000\n",
            "iter380000\n",
            "iter400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_m1 = acc(np.array(test_max_sims_artist) > 0.01, test_df['listened'])\n",
        "test_acc_m1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U761qiaPwJA3",
        "outputId": "c9f31882-caae-42e7-e40c-366f66fbe830"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.756675"
            ]
          },
          "metadata": {},
          "execution_count": 430
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_max_sims_track = collab_filter_predict(test_df['user'], test_df['track_name'], tracks_per_user, users_per_track, title_content_preds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLW0x6Hit2ts",
        "outputId": "ff505d15-7b53-4cb5-bae5-ce1da508ad1f"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 30000\n",
            "iter: 60000\n",
            "iter: 90000\n",
            "iter: 120000\n",
            "iter: 150000\n",
            "iter: 180000\n",
            "iter: 210000\n",
            "iter: 240000\n",
            "iter: 270000\n",
            "iter: 300000\n",
            "iter: 330000\n",
            "iter: 360000\n",
            "iter: 390000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_m2 = acc(np.array(test_max_sims_track) > 0.01, test_df['listened'])\n",
        "test_acc_m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaMYkEcwjkT",
        "outputId": "01eae559-7e66-4396-de00-147e771abffa"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6857975"
            ]
          },
          "metadata": {},
          "execution_count": 435
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_ensemble = acc(((acc_track * np.array(test_max_sims_track) + acc_artist * np.array(test_max_sims_artist) ) > new_val_threshold), test_df['listened'])\n",
        "test_acc_ensemble"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05Qb9M1xMOt",
        "outputId": "aa53a82f-4cb4-488d-d523-267400a2902b"
      },
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7641325"
            ]
          },
          "metadata": {},
          "execution_count": 437
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}