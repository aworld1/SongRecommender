{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling\n",
        "\n",
        "In this section, we propose several models to predict whether or not a song has been listened to. We begin with the implementation of several baselines, and proceed with a more advanced model."
      ],
      "metadata": {
        "id": "Rlf7-Ia7DFYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spotipy\n",
        "!pip install nimfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDhbyDqB-hh",
        "outputId": "8c7525da-99fc-466a-9f1d-6617c32329e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spotipy in /usr/local/lib/python3.7/dist-packages (2.21.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (1.26.13)\n",
            "Requirement already satisfied: redis>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from spotipy) (4.3.5)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.7/dist-packages (from spotipy) (2.28.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.1.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.0.2)\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.3->spotipy) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.3->spotipy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.3->spotipy) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.0->spotipy) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nimfa\n",
            "  Downloading nimfa-1.4.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.7.3)\n",
            "Installing collected packages: nimfa\n",
            "Successfully installed nimfa-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3IK293-hyAgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "import spotipy\n",
        "import nimfa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_relevant_ds(songs: list):\n",
        "    \"\"\"\n",
        "    Preprocesses data, simultaneously building relevant data structures\n",
        "    \n",
        "    @param data - a data list of playlist dictionaries to preprocess\n",
        "    @returns a list of tracks per user, users per track, watered down data list\n",
        "    \"\"\"\n",
        "    \n",
        "    def process_uri(uri:str):\n",
        "        \"\"\"URI Processing method\"\"\"\n",
        "        return uri.split(\":\")[2]\n",
        "        \n",
        "    print(\"Preprocessing started...\")\n",
        "    tracks_per_user, users_per_track, users_per_artist, artists_per_user = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "    \n",
        "    # Traversing through data and preprocessing\n",
        "    for song in songs:\n",
        "\n",
        "      # Obtaining user\n",
        "      user = song['user']\n",
        "\n",
        "      # obtaining necessary data\n",
        "      track, artist, album = song['track_name'], song['artist_name'], song['album_name']\n",
        "      \n",
        "      # Appending data to data structures\n",
        "      tracks_per_user[user].append(track)\n",
        "      users_per_track[track].append(user)\n",
        "      users_per_artist[artist].append(user)\n",
        "      artists_per_user[user].append(artist)\n",
        "            \n",
        "    return tracks_per_user, users_per_track, users_per_artist, artists_per_user\n",
        "            \n"
      ],
      "metadata": {
        "id": "VFEBc8coe7fp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in training and testing data\n",
        "with open('data_train.json', 'r') as train_reader:\n",
        "  data_train_val = json.load(train_reader)\n",
        "\n",
        "with open('data_test.json', 'r') as test_reader:\n",
        "  data_test= json.load(test_reader) "
      ],
      "metadata": {
        "id": "yl6pmXmgDpWG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_val = data_train_val[:600_000], data_train_val[600_000:]"
      ],
      "metadata": {
        "id": "kquJLueVIoiH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_tracks_per_user, val_users_per_track, val_users_per_artist, val_artists_per_user = build_relevant_ds(data_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tm-cp11h6by",
        "outputId": "28d7ce85-0afb-4055-cbbf-d443ddd00945"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation, Test Set Construction\n",
        "\n",
        "The Validation and Test set are balanced sets of positive and negative instances, maintaining 1/2 positive user-item pairs (the user listened to the song) and 1/2 negative user-item pairs (the user did not listen to the song)."
      ],
      "metadata": {
        "id": "OMZV5f21b4CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "val_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_val:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['track_name'] in val_tracks_per_user[user]:\n",
        "        random_song = data_val[np.random.randint(0, len(data_val))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "\n",
        "    # Group user with negative example\n",
        "    neg_ex['user'] = user\n",
        "    \n",
        "    # Appending\n",
        "    val_neg_ex.append(neg_ex)\n",
        "\n",
        "data_val += val_neg_ex"
      ],
      "metadata": {
        "id": "BNCSPXAHPCUS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tracks_per_user, test_users_per_track, test_users_per_artist, test_artists_per_user = build_relevant_ds(data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4gy-TSglKEe",
        "outputId": "24ef92a2-e4fd-4ba8-e058-1d3a71184965"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "test_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_test:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['track_name'] in test_tracks_per_user[user]:\n",
        "        random_song = data_test[np.random.randint(0, len(data_test))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "\n",
        "    # Group user with negative example\n",
        "    neg_ex['user'] = user\n",
        "    \n",
        "    # Appending\n",
        "    test_neg_ex.append(neg_ex)\n",
        "\n",
        "data_test += test_neg_ex"
      ],
      "metadata": {
        "id": "HmGyo6m_jrrw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing data as dataframe for easier use\n",
        "train_df = pd.DataFrame.from_records(data_train)\n",
        "val_df = pd.DataFrame.from_records(data_val)\n",
        "test_df = pd.DataFrame.from_records(data_test)"
      ],
      "metadata": {
        "id": "bs-xyAGYN7hr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Validation Length: {len(data_val)}, Test Length: {len(data_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcT7ukUJJuHi",
        "outputId": "f90de8ed-6cfb-491d-b455-b3f2bd998b0f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Length: 400000, Test Length: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Models\n",
        "\n",
        "We implement the following baseline models, and attempt to exceed the performance of all 3:\n",
        "\n",
        "1. Totally Naive Baseline: Prediction by Popularity\n",
        "2. Medium Tier Baseline: Collaborative Filtering\n",
        "3. Advanced Basline: Regression Modeling"
      ],
      "metadata": {
        "id": "3mEsLB_dC7gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary data structures\n",
        "tracks_per_user, users_per_track, users_per_artist, artists_per_user = build_relevant_ds(data_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6chZ25OgCqoU",
        "outputId": "da7b8a6b-30bb-49ef-a9b1-895431c3ca0f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivating Naive Basline: Prediction by Popularity (Unpersonalized Recommendation)"
      ],
      "metadata": {
        "id": "h5mQbLFOYPRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_by_pop(tracks: np.array, most_popular: set):\n",
        "  \"\"\"Predicts that a song was listened to if it was among the most popular songs\"\"\"\n",
        "  return [True if track in most_popular else False for track in tracks]\n",
        "\n",
        "def construct_most_popular(users_per_track: list) -> set:\n",
        "    \"\"\"Naive Baseline: Predicts track has been listened to by user if it's in the tracks that account for top 1/2 of listens\"\"\"\n",
        "    # Most popular list init\n",
        "    most_popular = []\n",
        "\n",
        "    # Sorting tracks by popularity\n",
        "    track_popularities = [(len(users_per_track[track]), track) for track in users_per_track]\n",
        "    track_popularities.sort(reverse = True)\n",
        "\n",
        "    # Computing half of total listens\n",
        "    half_tot_popularity = sum([len(users_per_track[track]) for track in users_per_track]) // 2\n",
        "    \n",
        "    # init cumulative popularity\n",
        "    cum_pop, counter = 0,0\n",
        "\n",
        "    # While haven't accounted for half of total listens\n",
        "    while(cum_pop < half_tot_popularity):\n",
        "      # Appending song and adjusting iterators\n",
        "      most_popular.append(track_popularities[counter][1])\n",
        "      cum_pop += track_popularities[counter][0]\n",
        "      counter += 1\n",
        "\n",
        "    return set(most_popular)\n",
        "\n",
        "\n",
        "def acc(labels: np.array, predictions: np.array):\n",
        "  \"\"\"Accuracy computation\"\"\"\n",
        "  return sum(predictions == labels) / len(labels)\n"
      ],
      "metadata": {
        "id": "F_PKbJKGEzbC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_popular = construct_most_popular(users_per_track)"
      ],
      "metadata": {
        "id": "PUaKjuifE2dh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = predict_by_pop(val_df['track_name'], most_popular)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5WToG0HKak",
        "outputId": "4a4519af-555b-4abe-92af-c358ba68cf1d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 0.50145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = predict_by_pop(test_df['track_name'], most_popular)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwHI4SILc0K",
        "outputId": "17570610-9d28-47c9-932b-acf7118390e0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5007125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions from extremely naive Baseline\n",
        "\n",
        "Due to the construction of our validation and test set, prediction by popularity is an entirely ineffective way to build a decision boundary. \n",
        "\n",
        "Although the top 55% (55% was the optimal validation parameter) of Popular songs comprise more of the positive instances, they also comprise more of the equivalent number of randomly sampled negative instances. \n",
        "\n",
        "Since we were just as likely to sample a popular song as a positive instance as we were to sample a popular song as a negative instance, prediction by popularity provides no new information. This motivates that all improvement over random chance can **only** be accounted for by personalization"
      ],
      "metadata": {
        "id": "sWdXWeibQbGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Personalization Basline: Prediction by Collaborative Filtering (Personalized Recommendation)"
      ],
      "metadata": {
        "id": "OzxD3Gl5dXeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(set1: set, set2: set):\n",
        "  \"\"\"Jaccard similarity metric\"\"\"\n",
        "  inter = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "  return inter / union if union !=0 else 0"
      ],
      "metadata": {
        "id": "JaSwpZ0BQoo8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collab_filter_predict(users: list, tracks: list, tracks_per_user: dict, users_per_track:dict):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar users\"\"\"\n",
        "  predictions = []\n",
        "  iter = 0\n",
        "  for track_user, track in zip(users, tracks):\n",
        "\n",
        "    # Similarity vector\n",
        "    similarities = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      similarities.append(jaccard(other_track_users, track_users))\n",
        "\n",
        "    predictions.append(True if (len(similarities) != 0 and max(similarities) > 0.005) else False)\n",
        "  \n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "L8ij6vyUa7iN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = collab_filter_predict(val_df['user'], val_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQS4eekHa8FT",
        "outputId": "264fc7d1-4122-472a-93f3-63e854fee5d7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 0.663725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = collab_filter_predict(test_df['user'], test_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxACLAOPa8Z2",
        "outputId": "52e3a480-c0f6-4295-b485-e50dd6fac9a6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6981475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This thresholded collaborative filtering already does pretty well! By leveraging the properties of the validation and test sets (half of the examples are positive, so we can utilize only our 200_000 most positive predictions) let's check if we can do better by utilizing this ranking approach!"
      ],
      "metadata": {
        "id": "3V4Ha81P0-BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collab_filter_predict_structurally(users: list, tracks: list, tracks_per_user: dict, users_per_track:dict):\n",
        "  \"\"\"Semi-Personalized Baseline: prediction on the basis of similar users\"\"\"\n",
        "  predictions = []\n",
        "  max_similarities = []\n",
        "  for track_user, track in zip(users, tracks):\n",
        "\n",
        "    # Similarity vector\n",
        "    sims = []\n",
        "\n",
        "    # Obtaining set of user other tracks\n",
        "    user_tracks = tracks_per_user[track_user]\n",
        "    track_users = set(users_per_track[track])\n",
        "\n",
        "    # Seeing if other tracks user listens to are similar\n",
        "    for external_track in user_tracks:\n",
        "\n",
        "      # Ensuring don't use track itself\n",
        "      if external_track == track:\n",
        "        continue\n",
        "\n",
        "      # Obtaining other track users\n",
        "      other_track_users = set(users_per_track[external_track])\n",
        "\n",
        "      # Computing cross track similarity\n",
        "      sims.append(jaccard(other_track_users, track_users))\n",
        "\n",
        "    # If other tracks exist for user, append compute\n",
        "    max_similarities.append(max(sims) if len(sims) > 0 else 0)\n",
        "  \n",
        "  sorted_max_sims = sorted(max_similarities, reverse=True)\n",
        "  confidence_thresh = sorted_max_sims[len(max_similarities)//2]\n",
        "\n",
        "  predictions = [True if sim > confidence_thresh else False for sim in max_similarities]\n",
        "  \n",
        "  return predictions"
      ],
      "metadata": {
        "id": "fogXbaW7a8kg"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = collab_filter_predict_structurally(val_df['user'], val_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uaBlK9sa85h",
        "outputId": "d0a2794d-db58-4540-e508-e5ca862e972a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 0.6425425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = collab_filter_predict_structurally(test_df['user'], test_df['track_name'], tracks_per_user, users_per_track)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE2zMnX5223V",
        "outputId": "03f614a3-5861-4a97-cd59-02b442486042"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.690045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somehow, adding structure did not make our model better. This could be due to several factors. For one, we suffer from a massive sparsity problem: many of our tracks have very few and hence only 0 overlap. This is part of the cold start problem: it is very unlikely that tracks with very few listeners will overlap with other tracks. We must figure out a better way to handle such tracks, as they are an integral part of our recommender system."
      ],
      "metadata": {
        "id": "nnyGVZI19rc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Basline: Regression Modeling\n",
        "Regression Modeling can utilize the same similarity factor as collaborative filtering, yet additionally accounts for the remainder of the data, building a model that can account both for user/item similarity and relevant features"
      ],
      "metadata": {
        "id": "hjv3TWTI6Bc3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "jE2oUNC-6g9j",
        "outputId": "3040ce12-06e7-4769-a340-22682b3ede75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cbd3ddfd6b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsnmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsnmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_vcol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlsnmf_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsnmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvrDc0SlPmGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}