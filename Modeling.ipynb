{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling\n",
        "\n",
        "In this section, we propose several models to predict whether or not a song has been listened to. We begin with the implementation of several baselines, and proceed with a more advanced model."
      ],
      "metadata": {
        "id": "Rlf7-Ia7DFYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nimfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDhbyDqB-hh",
        "outputId": "2391887d-7a68-4e10-d6b9-63ef15a7d17c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nimfa\n",
            "  Downloading nimfa-1.4.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from nimfa) (1.21.6)\n",
            "Installing collected packages: nimfa\n",
            "Successfully installed nimfa-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3IK293-hyAgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import nimfa as nf\n",
        "from collections import defaultdict\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in training and testing data\n",
        "with open('data_train.json', 'r') as train_reader:\n",
        "  data_train_val = json.load(train_reader)\n",
        "\n",
        "with open('data_test.json', 'r') as test_reader:\n",
        "  data_test= json.load(test_reader) "
      ],
      "metadata": {
        "id": "yl6pmXmgDpWG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_val = data_train_val[:600_000], data_train_val[600_000:]"
      ],
      "metadata": {
        "id": "kquJLueVIoiH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding negative records to validation data (already exist in test data)\n",
        "val_neg_ex = []\n",
        "\n",
        "# Sampling negatives\n",
        "for ex in data_val:\n",
        "    user = ex['user']\n",
        "    random_song = ex\n",
        "    \n",
        "    # Sampling random songs until one found from different playlist\n",
        "    while random_song['user'] == user:\n",
        "        random_song = data_val[np.random.randint(0, len(data_val))]\n",
        "    \n",
        "    # Negative example modification\n",
        "    neg_ex = copy.deepcopy(random_song)\n",
        "    neg_ex['listened'] = False\n",
        "    \n",
        "    # Appending\n",
        "    val_neg_ex.append(neg_ex)\n",
        "\n",
        "data_val += val_neg_ex"
      ],
      "metadata": {
        "id": "BNCSPXAHPCUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing data as dataframe for easier use\n",
        "train_df = pd.DataFrame.from_records(data_train)\n",
        "val_df = pd.DataFrame.from_records(data_val)\n",
        "test_df = pd.DataFrame.from_records(data_test)"
      ],
      "metadata": {
        "id": "bs-xyAGYN7hr"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Validation Length: {len(data_val)}, Test Length: {len(data_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcT7ukUJJuHi",
        "outputId": "39f962b1-b27e-44fa-ec23-3cf286b9e739"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Length: 400000, Test Length: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Models\n",
        "\n",
        "We implement the following baseline models, and attempt to exceed the performance of all 3:\n",
        "\n",
        "1. Naive Baseline: Prediction by Popularity\n",
        "2. Medium Tier Baseline: Collaborative Filtering\n",
        "3. Advanced Basline: Matrix Factorization"
      ],
      "metadata": {
        "id": "3mEsLB_dC7gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_relevant_ds(songs: list):\n",
        "    \"\"\"\n",
        "    Preprocesses data, simultaneously building relevant data structures\n",
        "    \n",
        "    @param data - a data list of playlist dictionaries to preprocess\n",
        "    @returns a list of tracks per user, users per track, watered down data list\n",
        "    \"\"\"\n",
        "    \n",
        "    def process_uri(uri:str):\n",
        "        \"\"\"URI Processing method\"\"\"\n",
        "        return uri.split(\":\")[2]\n",
        "        \n",
        "    print(\"Preprocessing started...\")\n",
        "    tracks_per_user, users_per_track, users_per_artist, artists_per_user = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "    \n",
        "    # Traversing through data and preprocessing\n",
        "    for song in songs:\n",
        "\n",
        "      # Obtaining user\n",
        "      user = song['user']\n",
        "\n",
        "      # obtaining necessary data\n",
        "      track, artist, album = song['track_name'], song['artist_name'], song['album_name']\n",
        "      \n",
        "      # Appending data to data structures\n",
        "      tracks_per_user[user].append(track)\n",
        "      users_per_track[track].append(user)\n",
        "      users_per_artist[artist].append(user)\n",
        "      artists_per_user[user].append(artist)\n",
        "            \n",
        "    return tracks_per_user, users_per_track, users_per_artist, artists_per_user\n",
        "            \n"
      ],
      "metadata": {
        "id": "lWMBowpCB9em"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary\n",
        "tracks_per_user, users_per_track, users_per_artist, artists_per_user = build_relevant_ds(data_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6chZ25OgCqoU",
        "outputId": "9cd248df-b434-4c8c-efcf-2ff03699303a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing started...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_by_pop(tracks: np.array, most_popular: set):\n",
        "  \"\"\"Predicts that a song was listened to if it was among the most popular songs\"\"\"\n",
        "  return [True if track in most_popular else False for track in tracks]\n",
        "\n",
        "def construct_most_popular(users_per_track: list) -> set:\n",
        "    \"\"\"Naive Baseline: Predicts track has been listened to by user if it's in the tracks that account for top 1/2 of listens\"\"\"\n",
        "    # Most popular list init\n",
        "    most_popular = []\n",
        "\n",
        "    # Sorting tracks by popularity\n",
        "    track_popularities = [(len(users_per_track[track]), track) for track in users_per_track]\n",
        "    track_popularities.sort(reverse = True)\n",
        "\n",
        "    # Computing half of total listens\n",
        "    half_tot_popularity = sum([len(users_per_track[track]) for track in users_per_track]) // 2\n",
        "    \n",
        "    # init cumulative popularity\n",
        "    cum_pop, counter = 0,0\n",
        "\n",
        "    # While haven't accounted for half of total listens\n",
        "    while(cum_pop < half_tot_popularity):\n",
        "      # Appending song and adjusting iterators\n",
        "      most_popular.append(track_popularities[counter][1])\n",
        "      cum_pop += track_popularities[counter][0]\n",
        "      counter += 1\n",
        "\n",
        "    return set(most_popular)\n",
        "\n",
        "\n",
        "def acc(labels: np.array, predictions: np.array):\n",
        "  \"\"\"Accuracy computation\"\"\"\n",
        "  return sum(predictions == labels) / len(labels)\n"
      ],
      "metadata": {
        "id": "F_PKbJKGEzbC"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_popular = construct_most_popular(users_per_track)"
      ],
      "metadata": {
        "id": "PUaKjuifE2dh"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_predictions = predict_by_pop(val_df['track_name'], most_popular)\n",
        "print(f\"Val Accuracy: {acc(val_df['listened'], val_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5WToG0HKak",
        "outputId": "2678d6cc-353b-425e-f8fd-9080a1394842"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 0.4996475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = predict_by_pop(test_df['track_name'], most_popular)\n",
        "print(f\"Test Accuracy: {acc(test_df['listened'], test_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwHI4SILc0K",
        "outputId": "823f0bcf-41b7-46fc-e379-fbd84793cba5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4973675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions from extremely naive Baseline\n",
        "\n",
        "Due to the construction of our validation and test set, prediction by popularity is an extremely ineffective way to approach our predictive task. By construction, our dataset"
      ],
      "metadata": {
        "id": "sWdXWeibQbGh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaSwpZ0BQoo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}